{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOP6iAfzcN8imo6V+5lJt5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuradovMaks/GradioInterfaceRNN/blob/main/Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UO-9c4xok45",
        "outputId": "a3d1a235-6321-4f6b-875d-9cb792f10d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.8.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.utils import get_file, to_categorical\n",
        "from keras.layers import Embedding, Flatten, Dense, SpatialDropout1D, BatchNormalization, Dropout, SimpleRNN\n",
        "from keras.layers import GRU, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import os   # Модуль для работы с файловой системой\n",
        "\n",
        "import glob # Вспомогательный модуль для работы с файловой системой\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fqtx_STtpuQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGdAJ8IkuYHi",
        "outputId": "b19fd1a2-ad6c-4d7b-c28f-c0302eda401d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-06 12:17:16--  https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53012480 (51M) [application/x-tar]\n",
            "Saving to: ‘navec_hudlit_v1_12B_500K_300d_100q.tar’\n",
            "\n",
            "navec_hudlit_v1_12B 100%[===================>]  50.56M  9.60MB/s    in 7.1s    \n",
            "\n",
            "2025-01-06 12:17:25 (7.16 MB/s) - ‘navec_hudlit_v1_12B_500K_300d_100q.tar’ saved [53012480/53012480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install navec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "batmjZbr4IGi",
        "outputId": "22ebff94-401e-4418-ef34-136616463f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting navec\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec) (1.26.4)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: navec\n",
            "Successfully installed navec-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from navec import Navec\n",
        "navec = Navec.load('navec_hudlit_v1_12B_500K_300d_100q.tar')"
      ],
      "metadata": {
        "id": "LV9wpEl34KC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = get_file(\n",
        "    \"russian_literature.zip\",\n",
        "    \"https://storage.yandexcloud.net/academy.ai/russian_literature.zip\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN_9ZZvJ4MU1",
        "outputId": "e880ed23-10df-4fb9-8ade-a0027465be24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.yandexcloud.net/academy.ai/russian_literature.zip\n",
            "\u001b[1m21547079/21547079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo '{data_path}' -d ./dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWOjpB4L4NmZ",
        "outputId": "c280553d-fae1-4dc8-caca-1992c26d2564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error:  cannot create ./dataset/poems/Blok/╨б╤В╨╕╤Е╨╛╤В╨▓╨╛╤А╨╡╨╜╨╕╤П 1897-1903 ╨│╨│, ╨╜╨╡ ╨▓╨╛╤И╨╡╨┤╤И╨╕╨╡ ╨▓ ╨╛╤Б╨╜╨╛╨▓╨╜╨╛╨╡ ╤Б╨╛╨▒╤А╨░╨╜╨╕╨╡.txt\n",
            "        File name too long\n",
            "error:  cannot create ./dataset/prose/Gogol/╨Я╨╛╨▓╨╡╤Б╤В╤М ╨╛ ╤В╨╛╨╝, ╨║╨░╨║ ╨┐╨╛╤Б╤Б╨╛╤А╨╕╨╗╤Б╤П ╨Ш╨▓╨░╨╜ ╨Ш╨▓╨░╨╜╨╛╨▓╨╕╤З ╤Б ╨Ш╨▓╨░╨╜╨╛╨╝ ╨Э╨╕╨║╨╕╤Д╨╛╤А╨╛╨▓╨╕╤З╨╡╨╝.txt\n",
            "        File name too long\n",
            "error:  cannot create ./dataset/publicism/Tolstoy/╨Ф╨╛╨║╨╗╨░╨┤, ╨┐╤А╨╕╨│╨╛╤В╨╛╨▓╨╗╨╡╨╜╨╜╤Л╨╣ ╨┤╨╗╤П ╨║╨╛╨╜╨│╤А╨╡╤Б╤Б╨░ ╨╛ ╨╝╨╕╤А╨╡ ╨▓ ╨б╤В╨╛╨║╨│╨╛╨╗╤М╨╝╨╡.txt\n",
            "        File name too long\n",
            "error:  cannot create ./dataset/publicism/Tolstoy/╨Я╨╛╤З╨╡╨╝╤Г ╤Е╤А╨╕╤Б╤В╨╕╨░╨╜╤Б╨║╨╕╨╡ ╨╜╨░╤А╨╛╨┤╤Л ╨▓╨╛╨╛╨▒╤Й╨╡ ╨╕ ╨▓ ╨╛╤Б╨╛╨▒╨╡╨╜╨╜╨╛╤Б╤В╨╕ ╤А╤Г╤Б╤Б╨║╨╕╨╣ ╨╜╨░╤Е╨╛╨┤╤П╤В╤Б╤П ╤В╨╡╨┐╨╡╤А╤М ╨▓ ╨▒╨╡╨┤╤Б╤В╨▓╨╡╨╜╨╜╨╛╨╝ ╨┐╨╛╨╗╨╛╨╢╨╡╨╜╨╕╨╕.txt\n",
            "        File name too long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_LIST = ['Turgenev','Dostoevsky','Bryusov','Tolstoy','Chekhov']"
      ],
      "metadata": {
        "id": "eIIsO1-_4PIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = {} #обьеденим текст\n",
        "\n",
        "for author in CLASS_LIST:\n",
        "  all_texts[author]= ''\n",
        "  for path in glob.glob('./dataset/poems/{}/*.txt'.format(author)) + glob.glob('./dataset/prose/{}/*.txt'.format(author)):\n",
        "    with open(path,'r',errors='ignore') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    all_texts[author]+=' ' + text.replace('\\n',' ')"
      ],
      "metadata": {
        "id": "WZXpB8fv4Qtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300    # размерность векторов эмбединга (300d в имени эмбединга)\n",
        "max_words = 15000      # Количество слов, рассматриваемых как признаки\n",
        "\n",
        "# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря\n",
        "tokenizer = Tokenizer(num_words=max_words,\n",
        "                      filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n",
        "                      lower=True, split=' ', char_level=False)\n",
        "\n",
        "\n",
        "# Построение частотного словаря по текстам\n",
        "tokenizer.fit_on_texts(all_texts.values())"
      ],
      "metadata": {
        "id": "dmevJl_e4S5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Преобразование текста в последовательность\n",
        "\n",
        "seq_train = tokenizer.texts_to_sequences(all_texts.values())"
      ],
      "metadata": {
        "id": "3lw18Vhc4U97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Балансировка датасета\n",
        "total_sum = sum(len(i) for i in seq_train)\n",
        "print(f'Датасет состоит из {total_sum} слов')\n",
        "\n",
        "print('Общая выборка по писателям (по словам)')\n",
        "\n",
        "mean_list = np.array([])\n",
        "\n",
        "for author in CLASS_LIST:\n",
        "  cls = CLASS_LIST.index(author)\n",
        "  print(f'{author} - {len(seq_train[cls])} слов, доля от общего текста: {len(seq_train[cls]) / total_sum * 100 :.2f} %')\n",
        "  mean_list = np.append(mean_list,len(seq_train[cls]))\n",
        "\n",
        "print('Среднее значение слов: ', np.round(mean_list.mean()))\n",
        "print('Медианное значение слов: ', np.median(mean_list))\n",
        "\n",
        "\n",
        "median = int(np.median(mean_list))\n",
        "CLASS_LIST_BALANCE = []\n",
        "seq_train_balance = []\n",
        "\n",
        "for author in CLASS_LIST:\n",
        "  cls = CLASS_LIST.index(author)\n",
        "  if len(seq_train[cls]) > median * 0.6:\n",
        "    CLASS_LIST_BALANCE.append(author)\n",
        "    seq_train_balance.append(seq_train[cls][:median])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GNK9TMg4VRr",
        "outputId": "dbae1ceb-4286-4292-de53-e07d253a246a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Датасет состоит из 3700125 слов\n",
            "Общая выборка по писателям (по словам)\n",
            "Turgenev - 305849 слов, доля от общего текста: 8.27 %\n",
            "Dostoevsky - 1555847 слов, доля от общего текста: 42.05 %\n",
            "Bryusov - 350266 слов, доля от общего текста: 9.47 %\n",
            "Tolstoy - 1243865 слов, доля от общего текста: 33.62 %\n",
            "Chekhov - 244298 слов, доля от общего текста: 6.60 %\n",
            "Среднее значение слов:  740025.0\n",
            "Медианное значение слов:  350266.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_balance = sum(len(i) for i in seq_train_balance)\n",
        "print(f'Выборка по писателям по словам {total_balance}')\n",
        "\n",
        "mean_list_balance = np.array([])\n",
        "for author in CLASS_LIST_BALANCE:\n",
        "  cls = CLASS_LIST_BALANCE.index(author)\n",
        "  print(f'{author} - {len(seq_train_balance[cls])} слов, доля  {len(seq_train_balance[cls]) / total_balance * 100 :.2f}%')\n",
        "  mean_list_balance = np.append(mean_list_balance,len(seq_train_balance[cls]))\n",
        "\n",
        "print('Среднее значение слов: ', np.round(mean_list_balance.mean()))\n",
        "print('Медианное значение слов: ', np.median(mean_list_balance))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUjZmI814Wy_",
        "outputId": "6f68ba76-1021-4132-8a63-29349474ac93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выборка по писателям по словам 1600945\n",
            "Turgenev - 305849 слов, доля  19.10%\n",
            "Dostoevsky - 350266 слов, доля  21.88%\n",
            "Bryusov - 350266 слов, доля  21.88%\n",
            "Tolstoy - 350266 слов, доля  21.88%\n",
            "Chekhov - 244298 слов, доля  15.26%\n",
            "Среднее значение слов:  320189.0\n",
            "Медианное значение слов:  350266.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Деление данных на выборки методом скользящего окна\n",
        "\n",
        "WIN_SIZE = 1000 # Ширина окна\n",
        "WiN_STEP = 100 #Шаг окна\n",
        "\n",
        "def seq_split(seq, win_size,win_step):\n",
        "  return[seq[i:i+ win_size] for i in range(0,len(seq) - win_size + 1,win_step)]\n",
        "\n",
        "\n",
        "\n",
        "def seq_vectorize(sequence_list,win_size,win_step,test_split,val_split,class_list):\n",
        "  x_train,y_train,x_val,y_val,x_test,y_test = [],[],[],[],[],[]\n",
        "  for author in class_list:\n",
        "    cls = class_list.index(author)\n",
        "    total_size = len(sequence_list[cls])\n",
        "    test_val_size = int((val_split + test_split) * total_size)\n",
        "    test_size = val_size = int(test_val_size / 2)\n",
        "    total_size = total_size - test_val_size\n",
        "\n",
        "    vectorize_train = seq_split(sequence_list[cls][:total_size],win_size,win_step)\n",
        "    vectorize_val = seq_split(sequence_list[cls][total_size:total_size + val_size],win_size,win_step)\n",
        "    vectorize_test = seq_split(sequence_list[cls][total_size+val_size:],win_size,win_step)\n",
        "\n",
        "    x_train+=vectorize_train\n",
        "    x_val += vectorize_val\n",
        "    x_test += vectorize_test\n",
        "\n",
        "    y_train += [to_categorical(cls,len(class_list))] * len(vectorize_train)\n",
        "    y_val += [to_categorical(cls,len(class_list))] * len(vectorize_val)\n",
        "    y_test += [to_categorical(cls,len(class_list))] * len(vectorize_test)\n",
        "\n",
        "  return np.array(x_train),np.array(y_train),np.array(x_val),np.array(y_val),np.array(x_test),np.array(y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "t0uVVQlL4bs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,y_train,x_val,y_val,x_test,y_test = seq_vectorize(seq_train_balance,WIN_SIZE,WiN_STEP,test_split=0.1,val_split=0.1,class_list=CLASS_LIST_BALANCE)"
      ],
      "metadata": {
        "id": "nm9f1SRV4eLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Загрузка весов в модель\n",
        "def load_embedding():\n",
        "  word_index = tokenizer.word_index\n",
        "  embedding_index = navec\n",
        "\n",
        "  embedding_matrix = np.zeros((max_words,embedding_dim))\n",
        "  for word,i in word_index.items():\n",
        "    if i < max_words:\n",
        "      embedding_vector = embedding_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "TRMhM3fV4fUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words,embedding_dim,input_shape=(WIN_SIZE,),weights=load_embedding()))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Bidirectional(LSTM(8,return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(8,return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(16,return_sequences=True,reset_after=True))\n",
        "model.add(GRU(16,reset_after=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(CLASS_LIST_BALANCE),activation='softmax'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYYZQTBd4iOZ",
        "outputId": "b0d5264a-c2b9-4818-dfb8-3e8946dd64f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "SLwta9Ib4nsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epochs,batch_size):\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_data=(x_val, y_val))\n",
        "  return f\"Обучение завершено. Финальная точность: {history.history['accuracy'][-1]}\""
      ],
      "metadata": {
        "id": "pDQRwwhK4pSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def predict(text):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tfidf_matrix = vectorizer.fit_transform([text])\n",
        "  y_pred = model.predict(tfidf_matrix)\n",
        "  pred_author = np.argmax(y_pred,axis=1)\n",
        "  return f\"Предопалагемый автор: {CLASS_LIST_BALANCE[int(pred_author)]}, с точностью {(y_pred[0][int(pred_author)]) * 100 :.2f} %\""
      ],
      "metadata": {
        "id": "_IXvaB5g5oXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Обучение и инференс модели\")\n",
        "\n",
        "    with gr.Tab(\"Обучение\"):\n",
        "        epochs = gr.Number(label=\"Количество эпох\", value=50)\n",
        "        batch_size = gr.Number(label=\"Размер батча\", value=128)\n",
        "        train_button = gr.Button(\"Обучить модель\")\n",
        "        train_output = gr.Textbox(label=\"Результат обучения\")\n",
        "\n",
        "        train_button.click(train_model, inputs=[epochs, batch_size], outputs=train_output)\n",
        "\n",
        "    with gr.Tab(\"Инференс\"):\n",
        "        input_data = gr.Textbox(label=\"Входные данные (через запятую)\")\n",
        "        predict_button = gr.Button(\"Предсказать\")\n",
        "        predict_output = gr.Textbox(label=\"Результат предсказания\")\n",
        "\n",
        "        predict_button.click(predict, inputs=input_data, outputs=predict_output)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "qp-yde1JL36E",
        "outputId": "f99682e0-98c5-405f-83de-e415dc1c4bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5228b615ac1e5e3e03.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5228b615ac1e5e3e03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}